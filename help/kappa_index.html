<p>This tool calculates the <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Kappa index of agreement</a> (KIA), or Cohen's Kappa, for two categorical input raster images (<code>input1</code> and <code>input2</code>). The KIA is a measure of inter-rater reliability (i.e. classification accuracy) and is widely applied in many fields, notably remote sensing. For example, The KIA is often used as a means of assessing the accuracy of an image classification analysis. The KIA can be interpreted as the percentage improvement that the underlying classification has over and above a random classifier (i.e. random assignment to categories). The user must specify the output HTML file (<code>output</code>). The input images must be of a categorical data type, i.e. contain classes. As a measure of classification accuracy, the KIA is more robust than the <em>overall percent agreement</em> because it takes into account the agreement occurring by chance. A KIA of 0 would indicate that the classifier is no better than random class assignment. In addition to the KIA, this tool will also output the <a href="http://gis.humboldt.edu/OLM/Courses/GSP_216_Online/lesson6-2/metrics.html">producer's and user's accuracy</a>, the overall accuracy, and the error matrix.</p> <h2>See Also</h2> <p><a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#cross_tabulation">cross_tabulation</a></p> <h2>Function Signature</h2> <p><code>def kappa_index(self, class_raster: Raster, reference_raster: Raster, output_html_file: str = "") -&gt; None: ... </code></p>
<h2>Project Links</h2>
<div align="left">
    <a href="https://www.whiteboxgeo.com/whitebox-workflows-for-python/">WbW Homepage</a>
    <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/preface.html">User Manual</a>
    <a href="https://www.whiteboxgeo.com/wbw-purchase/">Support WbW</a>
</div>        
