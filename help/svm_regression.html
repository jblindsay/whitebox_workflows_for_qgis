<h2>License Information</h2> <p>Use of this function requires a license for Whitebox Workflows for Python Professional (WbW-Pro). Please visit <a href="https://www.whiteboxgeo.com/">www.whiteboxgeo.com</a> to purchase a license.</p> <h2>Description</h2> <p>This tool performs a supervised <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">support vector machine (SVM) regression analysis</a> using multiple predictor rasters (<code>inputs</code>), or features, and training data (<code>training</code>). SVMs are a common class of supervised learning algorithms widely applied in many problem domains. This tool can be used to model the spatial distribution of continuous data, such as soil properties (e.g. percent sand/silt/clay). The training data take the form of an input vector Shapefile containing a set of points for which the known outcome data is contained within a field (<code>field</code>) of the attribute table. Each grid cell defines a stack of feature values (one value for each input raster), which serves as a point within the multi-dimensional feature space. Note that the <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#svm_classification">svm_classification</a> tool can be used to apply the SVM method to the modelling of categorical data.</p> <p>The user must specify the <em>c</em>-value (<code>-c</code>), the regularization parameter used in model optimization, the epsilon-value (<code>eps</code>), used in the development of the epsilon-SVM regression model, and the gamma-value (<code>gamma</code>), which is used in defining the radial basis function (Gaussian) kernel parameter.</p> <p>The tool splits the training data into two sets, one for training the model and one for testing the prediction. These test data are used to calculate the regression accuracy statistics, as well as to estimate the variable importance. The <code>test_proportion</code> parameter is used to set the proportion of the input training data used in model testing. For example, if <code>test_proportion = 0.2</code>, 20% of the training data will be set aside for testing, and this subset will be selected randomly. As a result of this random selection of test data, the tool behaves stochastically, and will result in a different model each time it is run.</p> <p>Note that the output image parameter (<code>output</code>) is optional. When unspecified, the tool will simply report the model accuracy statistics and variable importance, allowing the user to experiment with different parameter settings and input predictor raster combinations to optimize the model before applying it to model the outcome variable across the whole region defined by image data set.</p> <p>The SVM algorithm is based on the calculation of distances in multi-dimensional space. Feature scaling is essential to the application of SVM modelling, especially when the ranges of the features are different, for example, if they are measured in different units. Without scaling, features with larger ranges will have greater influence in computing the distances between points. The tool offers three options for feature-scaling (<code>scaling</code>), including 'None', 'Normalize', and 'Standardize'. Normalization simply rescales each of the features onto a 0-1 range. This is a good option for most applications, but it is highly sensitive to outliers because it is determined by the range of the minimum and maximum values. Standardization rescales predictors using their means and standard deviations, transforming the data into z-scores. This is a better option than normalization when you know that the data contain outlier values; however, it does does assume that the feature data are somewhat normally distributed, or are at least symmetrical in distribution.</p> <p>Because the SVM algorithm calculates distances in feature-space, like many other related algorithms, it suffers from the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_function">curse of dimensionality</a>. Distances become less meaningful in high-dimensional space because the vastness of these spaces means that distances between points are less significant (more similar). As such, if the predictor list includes insignificant or highly correlated variables, it is advisable to exclude these features during the model-building phase, or to use a dimension reduction technique such as <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#principal_component_analysis">principal_component_analysis</a> to transform the features into a smaller set of uncorrelated predictors.</p> <h2>Memory Usage</h2> <p>The peak memory usage of this tool is approximately 8 bytes per grid cell &times; # predictors.</p> <h2>See Also</h2> <p><a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#svm_classification">svm_classification</a>, <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#random_forest_regression">random_forest_regression</a>, <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#knn_regression">knn_regression</a>, <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/tool_help.html#principal_component_analysis">principal_component_analysis</a></p>
<h2>Project Links</h2>
<div align="left">
    <a href="https://www.whiteboxgeo.com/whitebox-workflows-for-python/">WbW Homepage</a>
    <a href="https://www.whiteboxgeo.com/manual/wbw-user-manual/book/preface.html">User Manual</a>
    <a href="https://www.whiteboxgeo.com/wbw-purchase/">Support WbW</a>
</div>        
